//go:build amd64 && asm && !purego

// ===========================================================================
// SSE2 Size-32 Radix-2 FFT Kernels for AMD64 (complex64)
// ===========================================================================
//
// Radix-2 FFT kernel for size 32.
//
// Stage 1 (radix-2): 16 butterflies (stride 1)
// Stage 2 (radix-2): 16 butterflies (stride 2)
// Stage 3 (radix-2): 16 butterflies (stride 4)
// Stage 4 (radix-2): 16 butterflies (stride 8)
// Stage 5 (radix-2): 16 butterflies (stride 16)
//
// ===========================================================================

#include "textflag.h"

// Forward transform, size 32, complex64, radix-2
TEXT ·ForwardSSE2Size32Radix2Complex64Asm(SB), NOSPLIT, $0-121
	// Load parameters
	MOVQ dst+0(FP), R8       // R8  = dst pointer
	MOVQ src+24(FP), R9      // R9  = src pointer
	MOVQ twiddle+48(FP), R10 // R10 = twiddle pointer
	MOVQ scratch+72(FP), R11 // R11 = scratch pointer
	MOVQ bitrev+96(FP), R12  // R12 = bitrev pointer
	MOVQ src+32(FP), R13     // R13 = n (should be 32)

	// Verify n == 32
	CMPQ R13, $32
	JNE  size32_r2_sse2_fwd_return_false

	// Validate slice lengths
	MOVQ dst+8(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_fwd_return_false

	MOVQ twiddle+56(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_fwd_return_false

	MOVQ scratch+80(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_fwd_return_false

	MOVQ bitrev+104(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_fwd_return_false

	// Select working buffer
	CMPQ R8, R9
	JNE  size32_r2_sse2_fwd_use_dst
	MOVQ R11, R8             // In-place: use scratch

size32_r2_sse2_fwd_use_dst:
	// ==================================================================
	// Bit-reversal permutation
	// ==================================================================
	XORQ CX, CX
size32_r2_sse2_fwd_bitrev_loop:
	MOVQ (R12)(CX*8), DX
	MOVSD (R9)(DX*8), X0
	MOVSD X0, (R8)(CX*8)
	INCQ CX
	CMPQ CX, $32
	JL   size32_r2_sse2_fwd_bitrev_loop

	// ==================================================================
	// Stage 1 & 2 (Combined)
	// Process 8 blocks of 4 elements.
	// ==================================================================
	MOVQ R8, SI              // SI = work buffer
	MOVQ $8, CX              // Loop counter
	MOVUPS ·maskNegHiPS(SB), X15 // Negate High mask (for -i)

size32_r2_sse2_fwd_stage12_loop:
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3

	// Stage 1 (stride 1, w=1)
	MOVAPS X0, X8
	ADDPS  X1, X8
	MOVAPS X0, X9
	SUBPS  X1, X9
	MOVAPS X8, X0
	MOVAPS X9, X1

	MOVAPS X2, X8
	ADDPS  X3, X8
	MOVAPS X2, X9
	SUBPS  X3, X9
	MOVAPS X8, X2
	MOVAPS X9, X3

	// Stage 2 (stride 2, w=[1, -i])
	MOVAPS X0, X8
	ADDPS  X2, X8
	MOVAPS X0, X9
	SUBPS  X2, X9
	MOVAPS X8, X0
	MOVAPS X9, X2

	MOVAPS X3, X10
	SHUFPS $0xB1, X10, X10
	XORPS  X15, X10          // t = X3 * -i
	MOVAPS X1, X8
	ADDPS  X10, X8
	MOVAPS X1, X9
	SUBPS  X10, X9
	MOVAPS X8, X1
	MOVAPS X9, X3

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)

	ADDQ $32, SI
	DECQ CX
	JNZ  size32_r2_sse2_fwd_stage12_loop

	// ==================================================================
	// Stage 3 (Stride 4)
	// Process 4 blocks of 8 elements.
	// Twiddles: 1, w4, w8, w12 (Indices into N=32: 0, 4, 8, 12)
	// ==================================================================
	MOVQ R8, SI
	MOVQ $4, CX

size32_r2_sse2_fwd_stage3_loop:
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3
	MOVSD 32(SI), X4
	MOVSD 40(SI), X5
	MOVSD 48(SI), X6
	MOVSD 56(SI), X7

	// Butterfly 0 (w0=1)
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X0, X9
	SUBPS  X4, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// Butterfly 1 (w4)
	MOVSD 32(R10), X10       // twiddle[4]
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11   // w.re
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12   // w.im
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// Butterfly 2 (w8 = -i)
	MOVAPS X6, X10
	SHUFPS $0xB1, X10, X10
	XORPS  X15, X10
	MOVAPS X2, X8
	ADDPS  X10, X8
	MOVAPS X2, X9
	SUBPS  X10, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// Butterfly 3 (w12)
	MOVSD 96(R10), X10       // twiddle[12]
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)
	MOVSD X4, 32(SI)
	MOVSD X5, 40(SI)
	MOVSD X6, 48(SI)
	MOVSD X7, 56(SI)

	ADDQ $64, SI
	DECQ CX
	JNZ  size32_r2_sse2_fwd_stage3_loop

	// ==================================================================
	// Stage 4 (Stride 8)
	// Process 2 blocks of 16 elements.
	// Twiddles: w0, w2, w4, w6, w8, w10, w12, w14
	// ==================================================================
	MOVQ R8, SI
	MOVQ $2, CX

size32_r2_sse2_fwd_stage4_loop:
	// Part 1: k=0..3
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3
	MOVSD 64(SI), X4
	MOVSD 72(SI), X5
	MOVSD 80(SI), X6
	MOVSD 88(SI), X7

	// k=0 (w0=1)
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X0, X9
	SUBPS  X4, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=1 (w2)
	MOVSD 16(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=2 (w4)
	MOVSD 32(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=3 (w6)
	MOVSD 48(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)
	MOVSD X4, 64(SI)
	MOVSD X5, 72(SI)
	MOVSD X6, 80(SI)
	MOVSD X7, 88(SI)

	// Part 2: k=4..7
	MOVSD 32(SI), X0
	MOVSD 40(SI), X1
	MOVSD 48(SI), X2
	MOVSD 56(SI), X3
	MOVSD 96(SI), X4
	MOVSD 104(SI), X5
	MOVSD 112(SI), X6
	MOVSD 120(SI), X7

	// k=4 (w8 = -i)
	MOVAPS X4, X10
	SHUFPS $0xB1, X10, X10
	XORPS  X15, X10
	MOVAPS X0, X8
	ADDPS  X10, X8
	MOVAPS X0, X9
	SUBPS  X10, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=5 (w10)
	MOVSD 80(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=6 (w12)
	MOVSD 96(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=7 (w14)
	MOVSD 112(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 32(SI)
	MOVSD X1, 40(SI)
	MOVSD X2, 48(SI)
	MOVSD X3, 56(SI)
	MOVSD X4, 96(SI)
	MOVSD X5, 104(SI)
	MOVSD X6, 112(SI)
	MOVSD X7, 120(SI)

	ADDQ $128, SI
	DECQ CX
	JNZ  size32_r2_sse2_fwd_stage4_loop

	// ==================================================================
	// Stage 5 (Stride 16)
	// 1 Block of 32 elements.
	// Twiddles: w0..w15
	// ==================================================================
	MOVQ R8, SI

	// Part 1: k=0..3
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3
	MOVSD 128(SI), X4
	MOVSD 136(SI), X5
	MOVSD 144(SI), X6
	MOVSD 152(SI), X7

	// k=0 (w0=1)
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X0, X9
	SUBPS  X4, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=1 (w1)
	MOVSD 8(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=2 (w2)
	MOVSD 16(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=3 (w3)
	MOVSD 24(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)
	MOVSD X4, 128(SI)
	MOVSD X5, 136(SI)
	MOVSD X6, 144(SI)
	MOVSD X7, 152(SI)

	// Part 2: k=4..7
	MOVSD 32(SI), X0
	MOVSD 40(SI), X1
	MOVSD 48(SI), X2
	MOVSD 56(SI), X3
	MOVSD 160(SI), X4
	MOVSD 168(SI), X5
	MOVSD 176(SI), X6
	MOVSD 184(SI), X7

	// k=4 (w4)
	MOVSD 32(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X4, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X4, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X0, X8
	ADDPS  X14, X8
	MOVAPS X0, X9
	SUBPS  X14, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=5 (w5)
	MOVSD 40(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=6 (w6)
	MOVSD 48(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=7 (w7)
	MOVSD 56(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 32(SI)
	MOVSD X1, 40(SI)
	MOVSD X2, 48(SI)
	MOVSD X3, 56(SI)
	MOVSD X4, 160(SI)
	MOVSD X5, 168(SI)
	MOVSD X6, 176(SI)
	MOVSD X7, 184(SI)

	// Part 3: k=8..11
	MOVSD 64(SI), X0
	MOVSD 72(SI), X1
	MOVSD 80(SI), X2
	MOVSD 88(SI), X3
	MOVSD 192(SI), X4
	MOVSD 200(SI), X5
	MOVSD 208(SI), X6
	MOVSD 216(SI), X7

	// k=8 (w8 = -i)
	MOVAPS X4, X10
	SHUFPS $0xB1, X10, X10
	XORPS  X15, X10
	MOVAPS X0, X8
	ADDPS  X10, X8
	MOVAPS X0, X9
	SUBPS  X10, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=9 (w9)
	MOVSD 72(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=10 (w10)
	MOVSD 80(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=11 (w11)
	MOVSD 88(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 64(SI)
	MOVSD X1, 72(SI)
	MOVSD X2, 80(SI)
	MOVSD X3, 88(SI)
	MOVSD X4, 192(SI)
	MOVSD X5, 200(SI)
	MOVSD X6, 208(SI)
	MOVSD X7, 216(SI)

	// Part 4: k=12..15
	MOVSD 96(SI), X0
	MOVSD 104(SI), X1
	MOVSD 112(SI), X2
	MOVSD 120(SI), X3
	MOVSD 224(SI), X4
	MOVSD 232(SI), X5
	MOVSD 240(SI), X6
	MOVSD 248(SI), X7

	// k=12 (w12)
	MOVSD 96(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X4, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X4, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X0, X8
	ADDPS  X14, X8
	MOVAPS X0, X9
	SUBPS  X14, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=13 (w13)
	MOVSD 104(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=14 (w14)
	MOVSD 112(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=15 (w15)
	MOVSD 120(R10), X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 96(SI)
	MOVSD X1, 104(SI)
	MOVSD X2, 112(SI)
	MOVSD X3, 120(SI)
	MOVSD X4, 224(SI)
	MOVSD X5, 232(SI)
	MOVSD X6, 240(SI)
	MOVSD X7, 248(SI)

	// Copy to dst if needed
	MOVQ dst+0(FP), R14
	CMPQ R8, R14
	JE   size32_r2_sse2_fwd_done

	// Copy 32 elements (16 XMM vectors)
	MOVUPS (R8), X0
	MOVUPS X0, (R14)
	MOVUPS 16(R8), X1
	MOVUPS X1, 16(R14)
	MOVUPS 32(R8), X2
	MOVUPS X2, 32(R14)
	MOVUPS 48(R8), X3
	MOVUPS X3, 48(R14)
	MOVUPS 64(R8), X4
	MOVUPS X4, 64(R14)
	MOVUPS 80(R8), X5
	MOVUPS X5, 80(R14)
	MOVUPS 96(R8), X6
	MOVUPS X6, 96(R14)
	MOVUPS 112(R8), X7
	MOVUPS X7, 112(R14)
	MOVUPS 128(R8), X8
	MOVUPS X8, 128(R14)
	MOVUPS 144(R8), X9
	MOVUPS X9, 144(R14)
	MOVUPS 160(R8), X10
	MOVUPS X10, 160(R14)
	MOVUPS 176(R8), X11
	MOVUPS X11, 176(R14)
	MOVUPS 192(R8), X12
	MOVUPS X12, 192(R14)
	MOVUPS 208(R8), X13
	MOVUPS X13, 208(R14)
	MOVUPS 224(R8), X14
	MOVUPS X14, 224(R14)
	MOVUPS 240(R8), X15
	MOVUPS X15, 240(R14)

size32_r2_sse2_fwd_done:
	MOVB $1, ret+120(FP)
	RET

size32_r2_sse2_fwd_return_false:
	MOVB $0, ret+120(FP)
	RET

// Inverse transform, size 32, complex64, radix-2
TEXT ·InverseSSE2Size32Radix2Complex64Asm(SB), NOSPLIT, $0-121
	// Load parameters
	MOVQ dst+0(FP), R8
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ bitrev+96(FP), R12
	MOVQ src+32(FP), R13

	// Verify n == 32
	CMPQ R13, $32
	JNE  size32_r2_sse2_inv_return_false

	// Validate slice lengths
	MOVQ dst+8(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_inv_return_false

	MOVQ twiddle+56(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_inv_return_false

	MOVQ scratch+80(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_inv_return_false

	MOVQ bitrev+104(FP), AX
	CMPQ AX, $32
	JL   size32_r2_sse2_inv_return_false

	// Select working buffer
	CMPQ R8, R9
	JNE  size32_r2_sse2_inv_use_dst
	MOVQ R11, R8

size32_r2_sse2_inv_use_dst:
	// Bit-reversal
	XORQ CX, CX
size32_r2_sse2_inv_bitrev_loop:
	MOVQ (R12)(CX*8), DX
	MOVSD (R9)(DX*8), X0
	MOVSD X0, (R8)(CX*8)
	INCQ CX
	CMPQ CX, $32
	JL   size32_r2_sse2_inv_bitrev_loop

	// Stage 1 & 2 (Combined)
	MOVQ R8, SI
	MOVQ $8, CX
	MOVUPS ·maskNegLoPS(SB), X15 // Negate Low mask (for i)

size32_r2_sse2_inv_stage12_loop:
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3

	// Stage 1
	MOVAPS X0, X8
	ADDPS  X1, X8
	MOVAPS X0, X9
	SUBPS  X1, X9
	MOVAPS X8, X0
	MOVAPS X9, X1

	MOVAPS X2, X8
	ADDPS  X3, X8
	MOVAPS X2, X9
	SUBPS  X3, X9
	MOVAPS X8, X2
	MOVAPS X9, X3

	// Stage 2 (w=[1, i])
	MOVAPS X0, X8
	ADDPS  X2, X8
	MOVAPS X0, X9
	SUBPS  X2, X9
	MOVAPS X8, X0
	MOVAPS X9, X2

	MOVAPS X3, X10
	SHUFPS $0xB1, X10, X10
	XORPS  X15, X10          // t = X3 * i
	MOVAPS X1, X8
	ADDPS  X10, X8
	MOVAPS X1, X9
	SUBPS  X10, X9
	MOVAPS X8, X1
	MOVAPS X9, X3

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)

	ADDQ $32, SI
	DECQ CX
	JNZ  size32_r2_sse2_inv_stage12_loop

	// Stage 3 (Stride 4)
	MOVQ R8, SI
	MOVQ $4, CX
	MOVUPS ·maskNegHiPS(SB), X15 // Load maskNegHiPS for conjugation

size32_r2_sse2_inv_stage3_loop:
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3
	MOVSD 32(SI), X4
	MOVSD 40(SI), X5
	MOVSD 48(SI), X6
	MOVSD 56(SI), X7

	// Butterfly 0 (w0=1)
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X0, X9
	SUBPS  X4, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// Butterfly 1 (conj(w4))
	MOVSD 32(R10), X10
	XORPS  X15, X10          // conjugate
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// Butterfly 2 (conj(w8) = i)
	MOVAPS X6, X10
	SHUFPS $0xB1, X10, X10
	MOVUPS ·maskNegLoPS(SB), X14
	XORPS  X14, X10
	MOVAPS X2, X8
	ADDPS  X10, X8
	MOVAPS X2, X9
	SUBPS  X10, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// Butterfly 3 (conj(w12))
	MOVSD 96(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)
	MOVSD X4, 32(SI)
	MOVSD X5, 40(SI)
	MOVSD X6, 48(SI)
	MOVSD X7, 56(SI)

	ADDQ $64, SI
	DECQ CX
	JNZ  size32_r2_sse2_inv_stage3_loop

	// Stage 4 (Stride 8)
	MOVQ R8, SI
	MOVQ $2, CX

size32_r2_sse2_inv_stage4_loop:
	// Part 1: k=0..3
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3
	MOVSD 64(SI), X4
	MOVSD 72(SI), X5
	MOVSD 80(SI), X6
	MOVSD 88(SI), X7

	// k=0 (w0=1)
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X0, X9
	SUBPS  X4, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=1 (conj(w2))
	MOVSD 16(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=2 (conj(w4))
	MOVSD 32(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=3 (conj(w6))
	MOVSD 48(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)
	MOVSD X4, 64(SI)
	MOVSD X5, 72(SI)
	MOVSD X6, 80(SI)
	MOVSD X7, 88(SI)

	// Part 2: k=4..7
	MOVSD 32(SI), X0
	MOVSD 40(SI), X1
	MOVSD 48(SI), X2
	MOVSD 56(SI), X3
	MOVSD 96(SI), X4
	MOVSD 104(SI), X5
	MOVSD 112(SI), X6
	MOVSD 120(SI), X7

	// k=4 (conj(w8)=i)
	MOVAPS X4, X10
	SHUFPS $0xB1, X10, X10
	MOVUPS ·maskNegLoPS(SB), X14
	XORPS  X14, X10
	MOVAPS X0, X8
	ADDPS  X10, X8
	MOVAPS X0, X9
	SUBPS  X10, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=5 (conj(w10))
	MOVSD 80(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=6 (conj(w12))
	MOVSD 96(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=7 (conj(w14))
	MOVSD 112(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 32(SI)
	MOVSD X1, 40(SI)
	MOVSD X2, 48(SI)
	MOVSD X3, 56(SI)
	MOVSD X4, 96(SI)
	MOVSD X5, 104(SI)
	MOVSD X6, 112(SI)
	MOVSD X7, 120(SI)

	ADDQ $128, SI
	DECQ CX
	JNZ  size32_r2_sse2_inv_stage4_loop

	// Stage 5 (Stride 16)
	MOVQ R8, SI

	// Part 1: k=0..3
	MOVSD (SI), X0
	MOVSD 8(SI), X1
	MOVSD 16(SI), X2
	MOVSD 24(SI), X3
	MOVSD 128(SI), X4
	MOVSD 136(SI), X5
	MOVSD 144(SI), X6
	MOVSD 152(SI), X7

	// k=0
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X0, X9
	SUBPS  X4, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=1 (conj(w1))
	MOVSD 8(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=2 (conj(w2))
	MOVSD 16(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=3 (conj(w3))
	MOVSD 24(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, (SI)
	MOVSD X1, 8(SI)
	MOVSD X2, 16(SI)
	MOVSD X3, 24(SI)
	MOVSD X4, 128(SI)
	MOVSD X5, 136(SI)
	MOVSD X6, 144(SI)
	MOVSD X7, 152(SI)

	// Part 2: k=4..7
	MOVSD 32(SI), X0
	MOVSD 40(SI), X1
	MOVSD 48(SI), X2
	MOVSD 56(SI), X3
	MOVSD 160(SI), X4
	MOVSD 168(SI), X5
	MOVSD 176(SI), X6
	MOVSD 184(SI), X7

	// k=4 (conj(w4))
	MOVSD 32(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X4, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X4, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X0, X8
	ADDPS  X14, X8
	MOVAPS X0, X9
	SUBPS  X14, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=5 (conj(w5))
	MOVSD 40(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=6 (conj(w6))
	MOVSD 48(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=7 (conj(w7))
	MOVSD 56(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 32(SI)
	MOVSD X1, 40(SI)
	MOVSD X2, 48(SI)
	MOVSD X3, 56(SI)
	MOVSD X4, 160(SI)
	MOVSD X5, 168(SI)
	MOVSD X6, 176(SI)
	MOVSD X7, 184(SI)

	// Part 3: k=8..11
	MOVSD 64(SI), X0
	MOVSD 72(SI), X1
	MOVSD 80(SI), X2
	MOVSD 88(SI), X3
	MOVSD 192(SI), X4
	MOVSD 200(SI), X5
	MOVSD 208(SI), X6
	MOVSD 216(SI), X7

	// k=8 (conj(w8)=i)
	MOVAPS X4, X10
	SHUFPS $0xB1, X10, X10
	MOVUPS ·maskNegLoPS(SB), X14
	XORPS  X14, X10
	MOVAPS X0, X8
	ADDPS  X10, X8
	MOVAPS X0, X9
	SUBPS  X10, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=9 (conj(w9))
	MOVSD 72(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=10 (conj(w10))
	MOVSD 80(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=11 (conj(w11))
	MOVSD 88(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 64(SI)
	MOVSD X1, 72(SI)
	MOVSD X2, 80(SI)
	MOVSD X3, 88(SI)
	MOVSD X4, 192(SI)
	MOVSD X5, 200(SI)
	MOVSD X6, 208(SI)
	MOVSD X7, 216(SI)

	// Part 4: k=12..15
	MOVSD 96(SI), X0
	MOVSD 104(SI), X1
	MOVSD 112(SI), X2
	MOVSD 120(SI), X3
	MOVSD 224(SI), X4
	MOVSD 232(SI), X5
	MOVSD 240(SI), X6
	MOVSD 248(SI), X7

	// k=12 (conj(w12))
	MOVSD 96(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X4, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X4, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X0, X8
	ADDPS  X14, X8
	MOVAPS X0, X9
	SUBPS  X14, X9
	MOVAPS X8, X0
	MOVAPS X9, X4

	// k=13 (conj(w13))
	MOVSD 104(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X5, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X5, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X1, X8
	ADDPS  X14, X8
	MOVAPS X1, X9
	SUBPS  X14, X9
	MOVAPS X8, X1
	MOVAPS X9, X5

	// k=14 (conj(w14))
	MOVSD 112(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X6, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X6, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X2, X8
	ADDPS  X14, X8
	MOVAPS X2, X9
	SUBPS  X14, X9
	MOVAPS X8, X2
	MOVAPS X9, X6

	// k=15 (conj(w15))
	MOVSD 120(R10), X10
	XORPS  X15, X10
	MOVAPS X10, X11
	SHUFPS $0x00, X11, X11
	MOVAPS X10, X12
	SHUFPS $0x55, X12, X12
	MOVAPS X7, X13
	SHUFPS $0xB1, X13, X13
	MOVAPS X7, X14
	MULPS  X11, X14
	MULPS  X12, X13
	ADDSUBPS X13, X14
	MOVAPS X3, X8
	ADDPS  X14, X8
	MOVAPS X3, X9
	SUBPS  X14, X9
	MOVAPS X8, X3
	MOVAPS X9, X7

	MOVSD X0, 96(SI)
	MOVSD X1, 104(SI)
	MOVSD X2, 112(SI)
	MOVSD X3, 120(SI)
	MOVSD X4, 224(SI)
	MOVSD X5, 232(SI)
	MOVSD X6, 240(SI)
	MOVSD X7, 248(SI)

	// Scale by 1/32
	MOVSS ·thirtySecond32(SB), X15
	SHUFPS $0x00, X15, X15
	
	MOVQ $16, CX             // 16 XMM vectors
	MOVQ R8, SI
size32_r2_sse2_inv_scale_loop:
	MOVUPS (SI), X0
	MULPS X15, X0
	MOVUPS X0, (SI)
	ADDQ $16, SI
	DECQ CX
	JNZ size32_r2_sse2_inv_scale_loop

	// Copy to dst if needed
	MOVQ dst+0(FP), R14
	CMPQ R8, R14
	JE   size32_r2_sse2_inv_done

	MOVQ $16, CX
	MOVQ R8, SI
size32_r2_sse2_inv_copy_loop:
	MOVUPS (SI), X0
	MOVUPS X0, (R14)
	ADDQ $16, SI
	ADDQ $16, R14
	DECQ CX
	JNZ size32_r2_sse2_inv_copy_loop

size32_r2_sse2_inv_done:
	MOVB $1, ret+120(FP)
	RET

size32_r2_sse2_inv_return_false:
	MOVB $0, ret+120(FP)
	RET
