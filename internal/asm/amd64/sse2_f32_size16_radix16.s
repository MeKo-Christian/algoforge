//go:build amd64 && asm && !purego

#include "textflag.h"

// ===========================================================================
// Forward transform, size 16, complex64, radix-16 (4x4) variant
// ===========================================================================
TEXT ·ForwardSSE2Size16Radix16Complex64Asm(SB), NOSPLIT, $0-121
	MOVQ dst+0(FP), R8
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ bitrev+96(FP), R12
	MOVQ src+32(FP), R13

	CMPQ R13, $16
	JNE  fwd_ret_false

	MOVUPS 0(R9), X0
	MOVUPS 16(R9), X1
	MOVUPS 32(R9), X2
	MOVUPS 48(R9), X3
	MOVUPS 64(R9), X4
	MOVUPS 80(R9), X5
	MOVUPS 96(R9), X6
	MOVUPS 112(R9), X7

	// Step 1: Column FFTs
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X2, X9
	ADDPS  X6, X9
	MOVAPS X0, X10
	SUBPS  X4, X10
	MOVAPS X2, X11
	SUBPS  X6, X11
	
	MOVAPS X8, X0
	ADDPS  X9, X0
	MOVAPS X8, X4
	SUBPS  X9, X4
	
	MOVAPS X11, X12
	SHUFPS $0xB1, X12, X12
	MOVUPS ·maskNegLoPS(SB), X13
	XORPS  X13, X12
	
	MOVAPS X10, X2
	SUBPS  X12, X2
	MOVAPS X10, X6
	ADDPS  X12, X6
	
	MOVAPS X1, X8
	ADDPS  X5, X8
	MOVAPS X3, X9
	ADDPS  X7, X9
	MOVAPS X1, X10
	SUBPS  X5, X10
	MOVAPS X3, X11
	SUBPS  X7, X11
	
	MOVAPS X8, X1
	ADDPS  X9, X1
	MOVAPS X8, X5
	SUBPS  X9, X5
	
	MOVAPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X13, X12
	
	MOVAPS X10, X3
	SUBPS  X12, X3
	MOVAPS X10, X7
	ADDPS  X12, X7

	// Step 2: Twiddles
	MOVUPS 0(R10), X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X2, X10
	MULPS  X8, X10
	MOVAPS X2, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X2
	
	MOVUPS 16(R10), X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X3, X10
	MULPS  X8, X10
	MOVAPS X3, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X3
	
	MOVSD  0(R10), X14
	MOVHPS 16(R10), X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X4, X10
	MULPS  X8, X10
	MOVAPS X4, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X4
	
	MOVSD  32(R10), X14
	MOVHPS 48(R10), X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X5, X10
	MULPS  X8, X10
	MOVAPS X5, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X5
	
	MOVSD  0(R10), X14
	MOVHPS 24(R10), X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X6, X10
	MULPS  X8, X10
	MOVAPS X6, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X6
	
	MOVSD  48(R10), X14
	MOVSD  8(R10), X15
	XORPS  X8, X8
	SUBPS  X15, X8
	MOVLHPS X8, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X7, X10
	MULPS  X8, X10
	MOVAPS X7, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X7

	// Step 3: Row FFTs
	MOVUPS ·maskNegLoPS(SB), X15
	
	// Row 0
	MOVAPS X0, X10
	ADDPS  X1, X10
	MOVAPS X0, X11
	SUBPS  X1, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8  // y0
	MOVAPS X10, X9
	SUBPS  X12, X9  // y2
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	SUBPS  X12, X13 // y1
	MOVAPS X11, X14
	ADDPS  X12, X14 // y3
	MOVAPS X8, X0
	UNPCKLPD X13, X0
	MOVAPS X9, X1
	UNPCKLPD X14, X1
	
	// Row 1
	MOVAPS X2, X10
	ADDPS  X3, X10
	MOVAPS X2, X11
	SUBPS  X3, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	SUBPS  X12, X13
	MOVAPS X11, X14
	ADDPS  X12, X14
	MOVAPS X8, X2
	UNPCKLPD X13, X2
	MOVAPS X9, X3
	UNPCKLPD X14, X3
	
	// Row 2
	MOVAPS X4, X10
	ADDPS  X5, X10
	MOVAPS X4, X11
	SUBPS  X5, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	SUBPS  X12, X13
	MOVAPS X11, X14
	ADDPS  X12, X14
	MOVAPS X8, X4
	UNPCKLPD X13, X4
	MOVAPS X9, X5
	UNPCKLPD X14, X5
	
	// Row 3
	MOVAPS X6, X10
	ADDPS  X7, X10
	MOVAPS X6, X11
	SUBPS  X7, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	SUBPS  X12, X13
	MOVAPS X11, X14
	ADDPS  X12, X14
	MOVAPS X8, X6
	UNPCKLPD X13, X6
	MOVAPS X9, X7
	UNPCKLPD X14, X7

	// Step 4: Transpose and Store
	MOVAPS X0, X8
	UNPCKLPD X2, X8
	MOVUPS X8, 0(R8)
	MOVAPS X0, X8
	UNPCKHPD X2, X8
	MOVUPS X8, 32(R8)
	MOVAPS X4, X8
	UNPCKLPD X6, X8
	MOVUPS X8, 16(R8)
	MOVAPS X4, X8
	UNPCKHPD X6, X8
	MOVUPS X8, 48(R8)
	MOVAPS X1, X8
	UNPCKLPD X3, X8
	MOVUPS X8, 64(R8)
	MOVAPS X1, X8
	UNPCKHPD X3, X8
	MOVUPS X8, 96(R8)
	MOVAPS X5, X8
	UNPCKLPD X7, X8
	MOVUPS X8, 80(R8)
	MOVAPS X5, X8
	UNPCKHPD X7, X8
	MOVUPS X8, 112(R8)

	MOVB $1, ret+120(FP)
	RET

fwd_ret_false:
	MOVB $0, ret+120(FP)
	RET


// ===========================================================================
// Inverse transform
// ===========================================================================
TEXT ·InverseSSE2Size16Radix16Complex64Asm(SB), NOSPLIT, $0-121
	MOVQ dst+0(FP), R8
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ bitrev+96(FP), R12
	MOVQ src+32(FP), R13

	CMPQ R13, $16
	JNE  inv_ret_false

	MOVUPS 0(R9), X0
	MOVUPS 16(R9), X1
	MOVUPS 32(R9), X2
	MOVUPS 48(R9), X3
	MOVUPS 64(R9), X4
	MOVUPS 80(R9), X5
	MOVUPS 96(R9), X6
	MOVUPS 112(R9), X7

	// Step 1: Vertical IFFT4
	MOVAPS X0, X8
	ADDPS  X4, X8
	MOVAPS X2, X9
	ADDPS  X6, X9
	MOVAPS X0, X10
	SUBPS  X4, X10
	MOVAPS X2, X11
	SUBPS  X6, X11
	MOVAPS X8, X0
	ADDPS  X9, X0
	MOVAPS X8, X4
	SUBPS  X9, X4
	MOVAPS X11, X12
	SHUFPS $0xB1, X12, X12
	MOVUPS ·maskNegLoPS(SB), X13
	XORPS  X13, X12
	MOVAPS X10, X2
	ADDPS  X12, X2
	MOVAPS X10, X6
	SUBPS  X12, X6
	MOVAPS X1, X8
	ADDPS  X5, X8
	MOVAPS X3, X9
	ADDPS  X7, X9
	MOVAPS X1, X10
	SUBPS  X5, X10
	MOVAPS X3, X11
	SUBPS  X7, X11
	MOVAPS X8, X1
	ADDPS  X9, X1
	MOVAPS X8, X5
	SUBPS  X9, X5
	MOVAPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X13, X12
	MOVAPS X10, X3
	ADDPS  X12, X3
	MOVAPS X10, X7
	SUBPS  X12, X7

	// Step 2: Twiddles (Conjugated)
	MOVUPS ·maskNegHiPS(SB), X15
	MOVUPS 0(R10), X14
	XORPS  X15, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X2, X10
	MULPS  X8, X10
	MOVAPS X2, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X2
	MOVUPS 16(R10), X14
	XORPS  X15, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X3, X10
	MULPS  X8, X10
	MOVAPS X3, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X3
	MOVSD  0(R10), X14
	MOVHPS 16(R10), X14
	XORPS  X15, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X4, X10
	MULPS  X8, X10
	MOVAPS X4, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X4
	MOVSD  32(R10), X14
	MOVHPS 48(R10), X14
	XORPS  X15, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X5, X10
	MULPS  X8, X10
	MOVAPS X5, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X5
	MOVSD  0(R10), X14
	MOVHPS 24(R10), X14
	XORPS  X15, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X6, X10
	MULPS  X8, X10
	MOVAPS X6, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X6
	MOVSD  48(R10), X14
	MOVSD  8(R10), X12
	XORPS  X8, X8
	SUBPS  X12, X8
	MOVLHPS X8, X14
	XORPS  X15, X14
	MOVAPS X14, X8
	SHUFPS $0xA0, X8, X8
	MOVAPS X14, X9
	SHUFPS $0xF5, X9, X9
	MOVAPS X7, X10
	MULPS  X8, X10
	MOVAPS X7, X11
	SHUFPS $0xB1, X11, X11
	MULPS  X9, X11
	ADDSUBPS X11, X10
	MOVAPS X10, X7

	// Step 3: Row IFFTs
	MOVUPS ·maskNegLoPS(SB), X15
	MOVAPS X0, X10
	ADDPS  X1, X10
	MOVAPS X0, X11
	SUBPS  X1, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	ADDPS  X12, X13 // y1 = D0 + i*D1
	MOVAPS X11, X14
	SUBPS  X12, X14 // y3 = D0 - i*D1
	MOVAPS X8, X0
	UNPCKLPD X13, X0
	MOVAPS X9, X1
	UNPCKLPD X14, X1
	// Repeat for others...
	MOVAPS X2, X10
	ADDPS  X3, X10
	MOVAPS X2, X11
	SUBPS  X3, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	ADDPS  X12, X13
	MOVAPS X11, X14
	SUBPS  X12, X14
	MOVAPS X8, X2
	UNPCKLPD X13, X2
	MOVAPS X9, X3
	UNPCKLPD X14, X3
	MOVAPS X4, X10
	ADDPS  X5, X10
	MOVAPS X4, X11
	SUBPS  X5, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	ADDPS  X12, X13
	MOVAPS X11, X14
	SUBPS  X12, X14
	MOVAPS X8, X4
	UNPCKLPD X13, X4
	MOVAPS X9, X5
	UNPCKLPD X14, X5
	MOVAPS X6, X10
	ADDPS  X7, X10
	MOVAPS X6, X11
	SUBPS  X7, X11
	MOVAPS X10, X12
	MOVHLPS X10, X12
	MOVAPS X10, X8
	ADDPS  X12, X8
	MOVAPS X10, X9
	SUBPS  X12, X9
	MOVAPS X11, X12
	MOVHLPS X11, X12
	SHUFPS $0xB1, X12, X12
	XORPS  X15, X12
	MOVAPS X11, X13
	ADDPS  X12, X13
	MOVAPS X11, X14
	SUBPS  X12, X14
	MOVAPS X8, X6
	UNPCKLPD X13, X6
	MOVAPS X9, X7
	UNPCKLPD X14, X7

	// Step 4: Scale and Transpose
	MOVSS  ·sixteenth32(SB), X15
	SHUFPS $0x00, X15, X15
	MOVAPS X0, X8
	UNPCKLPD X2, X8
	MULPS  X15, X8
	MOVUPS X8, 0(R8)
	MOVAPS X0, X8
	UNPCKHPD X2, X8
	MULPS  X15, X8
	MOVUPS X8, 32(R8)
	MOVAPS X4, X8
	UNPCKLPD X6, X8
	MULPS  X15, X8
	MOVUPS X8, 16(R8)
	MOVAPS X4, X8
	UNPCKHPD X6, X8
	MULPS  X15, X8
	MOVUPS X8, 48(R8)
	MOVAPS X1, X8
	UNPCKLPD X3, X8
	MULPS  X15, X8
	MOVUPS X8, 64(R8)
	MOVAPS X1, X8
	UNPCKHPD X3, X8
	MULPS  X15, X8
	MOVUPS X8, 96(R8)
	MOVAPS X5, X8
	UNPCKLPD X7, X8
	MULPS  X15, X8
	MOVUPS X8, 80(R8)
	MOVAPS X5, X8
	UNPCKHPD X7, X8
	MULPS  X15, X8
	MOVUPS X8, 112(R8)

	MOVB $1, ret+120(FP)
	RET

inv_ret_false:
	MOVB $0, ret+120(FP)
	RET


