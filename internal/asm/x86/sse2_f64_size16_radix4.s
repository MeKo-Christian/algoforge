//go:build 386 && asm && !purego

// ===========================================================================
// SSE2 Size-16 Radix-4 FFT Kernels for 386 (complex128)
// ===========================================================================
//
// Radix-4 DIT FFT kernels for size 16.
// Adapted for 386 (8 XMM registers).
//
// ===========================================================================

#include "textflag.h"

// func ForwardSSE2Size16Radix4Complex128Asm(dst, src, twiddle, scratch []complex128, bitrev []int) bool
TEXT ·ForwardSSE2Size16Radix4Complex128Asm(SB), NOSPLIT, $64-64
	// Load parameters
	MOVL dst+0(FP), AX
	MOVL src+12(FP), CX
	MOVL src+16(FP), DX

	CMPL DX, $16
	JNE  fwd_err

	// Select working buffer
	CMPL AX, CX
	JNE  fwd_use_dst
	MOVL scratch+36(FP), AX
	
fwd_use_dst:
	MOVL AX, 0(SP)

	// Bit reversal
	MOVL bitrev+48(FP), DX
	XORL SI, SI
bitrev_loop:
	MOVL (DX)(SI*4), BX
	SHLL $4, BX
	MOVUPD (CX)(BX*1), X0
	MOVL 0(SP), DI
	MOVL SI, BX
	SHLL $4, BX
	MOVUPD X0, (DI)(BX*1)
	INCL SI
	CMPL SI, $16
	JL   bitrev_loop

	// ==================================================================
	// Stage 1: 4 butterflies
	// ==================================================================
	MOVL 0(SP), DI
	XORL SI, SI
	MOVUPS ·maskNegHiPD(SB), X7
	MOVUPS X7, 16(SP) // Store maskNegHiPD on stack

stage1_loop:
	LEAL (DI)(SI*1), BX
	MOVUPD 0(BX), X0
	MOVUPD 16(BX), X1
	MOVUPD 32(BX), X2
	MOVUPD 48(BX), X3

	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4 
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5 
	// t3 * -i
	MOVAPD X5, X6; SHUFPD $1, X6, X6; XORPD 16(SP), X6
	// y0, y1, y2, y3
	MOVAPD X0, X2; ADDPD X1, X2; SUBPD X1, X0 
	MOVAPD X4, X3; ADDPD X6, X3; SUBPD X6, X4 
	MOVUPD X2, 0(BX); MOVUPD X3, 16(BX); MOVUPD X0, 32(BX); MOVUPD X4, 48(BX)

	ADDL $64, SI
	CMPL SI, $256
	JL   stage1_loop

	// ==================================================================
	// Stage 2: 4 butterflies, distance 4
	// ==================================================================
	MOVUPS ·maskNegLoPD(SB), X6
	MOVUPS X6, 32(SP) // Store maskNegLoPD on stack

	// BF 0: w=1
	MOVL 0(SP), DI
	MOVUPD 0(DI), X0
	MOVUPD 64(DI), X1
	MOVUPD 128(DI), X2
	MOVUPD 192(DI), X3
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5
	SHUFPD $1, X5, X5; XORPD 16(SP), X5 // t3*-i
	MOVAPD X0, X2; ADDPD X1, X0; SUBPD X1, X2
	MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X0, 0(DI); MOVUPD X3, 64(DI); MOVUPD X2, 128(DI); MOVUPD X4, 192(DI)

	// BF 1
	MOVL twiddle+24(FP), CX
	MOVUPD 16(DI), X0
	MOVUPD 80(DI), X1
	MOVUPD 144(DI), X2
	MOVUPD 208(DI), X3
	
	// x1*w1
	MOVUPD 16(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X1, X6; SHUFPD $1, X6, X6; MULPD X5, X1; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X1
	// x2*w2
	MOVUPD 32(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X2, X6; SHUFPD $1, X6, X6; MULPD X5, X2; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X2
	// x3*w3
	MOVUPD 48(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X3, X6; SHUFPD $1, X6, X6; MULPD X5, X3; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X3
	// BF
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5; SHUFPD $1, X5, X5; XORPD 16(SP), X5
	MOVAPD X0, X2; ADDPD X1, X0; SUBPD X1, X2; MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X0, 16(DI); MOVUPD X3, 80(DI); MOVUPD X2, 144(DI); MOVUPD X4, 208(DI)

	// BF 2
	MOVUPD 32(DI), X0
	MOVUPD 96(DI), X1
	MOVUPD 160(DI), X2
	MOVUPD 224(DI), X3
	// x1*w2
	MOVUPD 32(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X1, X6; SHUFPD $1, X6, X6; MULPD X5, X1; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X1
	// x2*w4
	MOVUPD 64(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X2, X6; SHUFPD $1, X6, X6; MULPD X5, X2; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X2
	// x3*w6
	MOVUPD 96(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X3, X6; SHUFPD $1, X6, X6; MULPD X5, X3; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X3
	// BF
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5; SHUFPD $1, X5, X5; XORPD 16(SP), X5
	MOVAPD X0, X2; ADDPD X1, X0; SUBPD X1, X2; MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X0, 32(DI); MOVUPD X3, 96(DI); MOVUPD X2, 160(DI); MOVUPD X4, 224(DI)

	// BF 3
	MOVUPD 48(DI), X0
	MOVUPD 112(DI), X1
	MOVUPD 176(DI), X2
	MOVUPD 240(DI), X3
	// x1*w3
	MOVUPD 48(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X1, X6; SHUFPD $1, X6, X6; MULPD X5, X1; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X1
	// x2*w6
	MOVUPD 96(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X2, X6; SHUFPD $1, X6, X6; MULPD X5, X2; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X2
	// x3*w9
	MOVUPD 144(CX), X4
	MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X3, X6; SHUFPD $1, X6, X6; MULPD X5, X3; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X3
	// BF
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5; SHUFPD $1, X5, X5; XORPD 16(SP), X5
	MOVAPD X0, X2; ADDPD X1, X0; SUBPD X1, X2; MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X0, 48(DI); MOVUPD X3, 112(DI); MOVUPD X2, 176(DI); MOVUPD X4, 240(DI)

	// Copy back
	MOVL dst+0(FP), AX
	MOVL 0(SP), CX
	CMPL AX, CX
	JE   fwd_done
	XORL SI, SI
fwd_copy:
	MOVUPD (CX)(SI*1), X0; MOVUPD X0, (AX)(SI*1)
	ADDL $16, SI; CMPL SI, $256; JL   fwd_copy

fwd_done:
	MOVB $1, ret+60(FP)
	RET
fwd_err:
	MOVB $0, ret+60(FP)
	RET

// func InverseSSE2Size16Radix4Complex128Asm(dst, src, twiddle, scratch []complex128, bitrev []int) bool
TEXT ·InverseSSE2Size16Radix4Complex128Asm(SB), NOSPLIT, $64-64
	MOVL dst+0(FP), AX
	MOVL src+12(FP), CX
	MOVL src+16(FP), DX
	CMPL DX, $16
	JNE  inv_err

	CMPL AX, CX
	JNE  inv_use_dst
	MOVL scratch+36(FP), AX
inv_use_dst:
	MOVL AX, 0(SP)

	// Bit reversal
	MOVL bitrev+48(FP), DX
	XORL SI, SI
inv_bitrev_loop:
	MOVL (DX)(SI*4), BX
	SHLL $4, BX
	MOVUPD (CX)(BX*1), X0
	MOVL 0(SP), DI
	MOVL SI, BX
	SHLL $4, BX
	MOVUPD X0, (DI)(BX*1)
	INCL SI
	CMPL SI, $16
	JL   inv_bitrev_loop

	MOVUPS ·maskNegHiPD(SB), X0; MOVUPS X0, 16(SP)
	MOVUPS ·maskNegLoPD(SB), X0; MOVUPS X0, 32(SP)

	// Stage 1 (inv)
	MOVL 0(SP), DI
	XORL SI, SI
stage1_inv_loop:
	LEAL (DI)(SI*1), BX
	MOVUPD 0(BX), X0; MOVUPD 16(BX), X1; MOVUPD 32(BX), X2; MOVUPD 48(BX), X3
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5
	SHUFPD $1, X5, X5; XORPD 32(SP), X5 // t3*i
	MOVAPD X0, X2; ADDPD X1, X2; SUBPD X1, X0
	MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X2, 0(BX); MOVUPD X3, 16(BX); MOVUPD X0, 32(BX); MOVUPD X4, 48(BX)
	ADDL $64, SI; CMPL SI, $256; JL stage1_inv_loop

	// Stage 2 (inv)
	// BF 0
	MOVUPD 0(DI), X0; MOVUPD 64(DI), X1; MOVUPD 128(DI), X2; MOVUPD 192(DI), X3
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5
	SHUFPD $1, X5, X5; XORPD 32(SP), X5
	MOVAPD X0, X2; ADDPD X1, X2; SUBPD X1, X0
	MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X2, 0(DI); MOVUPD X3, 64(DI); MOVUPD X0, 128(DI); MOVUPD X4, 192(DI)

	MOVL twiddle+24(FP), CX
	// BF 1
	MOVUPD 16(DI), X0; MOVUPD 80(DI), X1; MOVUPD 144(DI), X2; MOVUPD 208(DI), X3
	// x1*conj(w1)
	MOVUPD 16(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X1, X6; SHUFPD $1, X6, X6; MULPD X5, X1; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X1
	// x2*conj(w2)
	MOVUPD 32(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X2, X6; SHUFPD $1, X6, X6; MULPD X5, X2; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X2
	// x3*conj(w3)
	MOVUPD 48(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X3, X6; SHUFPD $1, X6, X6; MULPD X5, X3; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X3
	// BF
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5; SHUFPD $1, X5, X5; XORPD 32(SP), X5
	MOVAPD X0, X2; ADDPD X1, X2; SUBPD X1, X0; MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X2, 16(DI); MOVUPD X3, 80(DI); MOVUPD X0, 144(DI); MOVUPD X4, 208(DI)

	// BF 2
	MOVUPD 32(DI), X0; MOVUPD 96(DI), X1; MOVUPD 160(DI), X2; MOVUPD 224(DI), X3
	// x1*conj(w2)
	MOVUPD 32(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X1, X6; SHUFPD $1, X6, X6; MULPD X5, X1; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X1
	// x2*conj(w4)
	MOVUPD 64(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X2, X6; SHUFPD $1, X6, X6; MULPD X5, X2; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X2
	// x3*conj(w6)
	MOVUPD 96(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X3, X6; SHUFPD $1, X6, X6; MULPD X5, X3; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X3
	// BF
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5; SHUFPD $1, X5, X5; XORPD 32(SP), X5
	MOVAPD X0, X2; ADDPD X1, X2; SUBPD X1, X0; MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X2, 32(DI); MOVUPD X3, 96(DI); MOVUPD X0, 160(DI); MOVUPD X4, 224(DI)

	// BF 3
	MOVUPD 48(DI), X0; MOVUPD 112(DI), X1; MOVUPD 176(DI), X2; MOVUPD 240(DI), X3
	// x1*conj(w3)
	MOVUPD 48(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X1, X6; SHUFPD $1, X6, X6; MULPD X5, X1; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X1
	// x2*conj(w6)
	MOVUPD 96(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X2, X6; SHUFPD $1, X6, X6; MULPD X5, X2; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X2
	// x3*conj(w9)
	MOVUPD 144(CX), X4; XORPD 16(SP), X4; MOVAPD X4, X5; SHUFPD $0, X5, X5; SHUFPD $3, X4, X4
	MOVAPD X3, X6; SHUFPD $1, X6, X6; MULPD X5, X3; MULPD X4, X6; XORPD 32(SP), X6; ADDPD X6, X3
	// BF
	MOVAPD X0, X4; ADDPD X2, X0; SUBPD X2, X4
	MOVAPD X1, X5; ADDPD X3, X1; SUBPD X3, X5; SHUFPD $1, X5, X5; XORPD 32(SP), X5
	MOVAPD X0, X2; ADDPD X1, X2; SUBPD X1, X0; MOVAPD X4, X3; ADDPD X5, X3; SUBPD X5, X4
	MOVUPD X2, 48(DI); MOVUPD X3, 112(DI); MOVUPD X0, 176(DI); MOVUPD X4, 240(DI)

	// Scale and Copy
	MOVL dst+0(FP), AX
	MOVL 0(SP), CX
	MOVSD ·sixteenth64(SB), X7; SHUFPD $0, X7, X7
	XORL SI, SI
inv_scale:
	MOVUPD (CX)(SI*1), X0; MULPD X7, X0; MOVUPD X0, (AX)(SI*1)
	ADDL $16, SI; CMPL SI, $256; JL   inv_scale

	MOVB $1, ret+60(FP)
	RET
inv_err:
	MOVB $0, ret+60(FP)
	RET
