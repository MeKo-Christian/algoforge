//go:build amd64 && asm && !purego

package kernels

import (
	"github.com/MeKo-Christian/algo-fft/internal/asm/amd64"
)

// Precomputed bit-reversal indices for size 64 (radix-4).
// This avoids allocation on every call.
// Generated by ComputeBitReversalIndicesRadix4(64).
var bitrev64Radix4 = [64]int{
	0, 16, 32, 48, 4, 20, 36, 52, 8, 24, 40, 56, 12, 28, 44, 60,
	1, 17, 33, 49, 5, 21, 37, 53, 9, 25, 41, 57, 13, 29, 45, 61,
	2, 18, 34, 50, 6, 22, 38, 54, 10, 26, 42, 58, 14, 30, 46, 62,
	3, 19, 35, 51, 7, 23, 39, 55, 11, 27, 43, 59, 15, 31, 47, 63,
}

// forwardDIT4096SixStepAVX2Complex64 computes a 4096-point forward FFT using the
// six-step (64×64 matrix) algorithm with AVX2-accelerated operations.
//
// This implementation uses:
// - AVX2 assembly for transpose operations (Steps 1, 6)
// - AVX2 assembly for fused transpose+twiddle (Steps 3+4)
// - Existing ForwardAVX2Size64Radix4Complex64Asm kernel for row FFTs (Steps 2, 5)
func forwardDIT4096SixStepAVX2Complex64(dst, src, twiddle, scratch []complex64, bitrev []int) bool {
	const (
		n = 4096
		m = 64 // sqrt(4096)
	)

	if len(dst) < n || len(twiddle) < n || len(scratch) < n || len(bitrev) < n || len(src) < n {
		return false
	}

	// Work buffer
	work := scratch[:n]

	// Step 1: Transpose src → work (AVX2 accelerated)
	if !amd64.Transpose64x64Complex64AVX2Asm(work, src) {
		return false
	}

	// Precompute row twiddles for size-64 FFT (stride by 64 to get W_64^k from W_4096^(k*64))
	var rowTwiddle [64]complex64
	for k := 0; k < m; k++ {
		rowTwiddle[k] = twiddle[k*m]
	}

	var rowScratch [64]complex64

	// Step 2: Row FFTs using AVX2 (64 FFTs of size 64)
	for r := 0; r < m; r++ {
		row := work[r*m : (r+1)*m]
		if !amd64.ForwardAVX2Size64Radix4Complex64Asm(row, row, rowTwiddle[:], rowScratch[:], bitrev64Radix4[:]) {
			return false
		}
	}

	// Steps 3+4 fused: Transpose and twiddle multiply (AVX2 accelerated)
	// dst[i*m+j] = work[j*m+i] * W_4096^(i*j)
	if !amd64.TransposeTwiddle64x64Complex64AVX2Asm(dst, work, twiddle) {
		return false
	}

	// Step 5: Row FFTs using AVX2 (64 FFTs of size 64)
	for r := 0; r < m; r++ {
		row := dst[r*m : (r+1)*m]
		if !amd64.ForwardAVX2Size64Radix4Complex64Asm(row, row, rowTwiddle[:], rowScratch[:], bitrev64Radix4[:]) {
			return false
		}
	}

	// Step 6: Final transpose dst → work → dst (AVX2 accelerated)
	if !amd64.Transpose64x64Complex64AVX2Asm(work, dst) {
		return false
	}

	copy(dst[:n], work)

	return true
}

// inverseDIT4096SixStepAVX2Complex64 computes a 4096-point inverse FFT using the
// six-step (64×64 matrix) algorithm with AVX2-accelerated operations.
func inverseDIT4096SixStepAVX2Complex64(dst, src, twiddle, scratch []complex64, bitrev []int) bool {
	const (
		n = 4096
		m = 64
	)

	if len(dst) < n || len(twiddle) < n || len(scratch) < n || len(bitrev) < n || len(src) < n {
		return false
	}

	work := scratch[:n]

	// Step 1: Transpose src → work (AVX2 accelerated)
	if !amd64.Transpose64x64Complex64AVX2Asm(work, src) {
		return false
	}

	// Precompute row twiddles
	var rowTwiddle [64]complex64
	for k := 0; k < m; k++ {
		rowTwiddle[k] = twiddle[k*m]
	}

	var rowScratch [64]complex64

	// Step 2: Row IFFTs using AVX2
	for r := 0; r < m; r++ {
		row := work[r*m : (r+1)*m]
		if !amd64.InverseAVX2Size64Radix4Complex64Asm(row, row, rowTwiddle[:], rowScratch[:], bitrev64Radix4[:]) {
			return false
		}
	}

	// Steps 3+4 fused: Transpose and conjugate twiddle multiply (AVX2 accelerated)
	// dst[i*m+j] = work[j*m+i] * conj(W_4096^(i*j))
	if !amd64.TransposeTwiddleConj64x64Complex64AVX2Asm(dst, work, twiddle) {
		return false
	}

	// Step 5: Row IFFTs using AVX2
	for r := 0; r < m; r++ {
		row := dst[r*m : (r+1)*m]
		if !amd64.InverseAVX2Size64Radix4Complex64Asm(row, row, rowTwiddle[:], rowScratch[:], bitrev64Radix4[:]) {
			return false
		}
	}

	// Step 6: Final transpose dst → work → dst (AVX2 accelerated)
	if !amd64.Transpose64x64Complex64AVX2Asm(work, dst) {
		return false
	}

	copy(dst[:n], work)

	return true
}
